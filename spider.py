# main.py - ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    import sys
    
    # æµ‹è¯•æ¨¡å¼ï¼šå¿«é€Ÿè°ƒè¯•é€‰æ‹©å™¨
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        print("ğŸ” æµ‹è¯•æ¨¡å¼ï¼šåªæŠ“å–ç¬¬ä¸€é¡µï¼Œå¼€å¯è¯¦ç»†æ—¥å¿—")
        config = SpiderConfig(
            keywords=["Python"],
            max_pages=1,
            items_per_page=5,  # åªæŠ“5æ¡
            headless=False,
            save_html=True,    # ä¿å­˜HTMLç”¨äºåˆ†æ
            save_csv=True,
            save_json=False,
            min_delay=2.0,
            max_delay=3.0
        )
    else:
        # æ­£å¸¸æ¨¡å¼
        config = SpiderConfig(
            keywords=["Python"],  # å¯ä»¥æœç´¢å¤šä¸ªå…³é”®è¯: ["Python", "æ•°æ®åˆ†æ", "Java"]
            max_pages=3,          # æ¯ä¸ªå…³é”®è¯æŠ“å– 3 é¡µ
            items_per_page=30,
            headless=False,       # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            min_delay=3.0,
            max_delay=6.0,
            save_csv=True,
            save_json=True,
            save_html=False       # ä¸ä¿å­˜é¡µé¢æºç ï¼ˆæ–‡ä»¶å¤ªå¤§ï¼‰
        )
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = BossSpider(config)
    
    # è¿è¡Œ
    spider.run()


# ============ ç‹¬ç«‹çš„é€‰æ‹©å™¨è°ƒè¯•å·¥å…· ============
"""
ä½¿ç”¨æ–¹æ³•ï¼š
python boss_spider.py debug

è¿™ä¼šæ‰“å¼€æµè§ˆå™¨å¹¶æ‰“å°å‡ºæ‰€æœ‰å¯èƒ½çš„é€‰æ‹©å™¨ï¼Œå¸®åŠ©ä½ æ‰¾åˆ°æ­£ç¡®çš„ class åç§°
"""

def debug_selectors():
    """è°ƒè¯•æ¨¡å¼ï¼šå¸®åŠ©æ‰¾åˆ°æ­£ç¡®çš„é€‰æ‹©å™¨"""
    print("\n" + "="*60)
    print("ğŸ”§ é€‰æ‹©å™¨è°ƒè¯•å·¥å…·")
    print("="*60)
    
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=False,
            executable_path=r"C:\Program Files\Google\Chrome\Application\chrome.exe"
        )
        page = browser.new_page()
        
        print("\n1. è®¿é—® Boss ç›´è˜...")
        page.goto("https://www.zhipin.com")
        time.sleep(5)
        # config.py - é…ç½®æ–‡ä»¶
import os
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class SpiderConfig:
    """çˆ¬è™«é…ç½®"""
    # æµè§ˆå™¨é…ç½®
    chrome_path: str = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
    headless: bool = False
    viewport: Dict = None
    
    # æœç´¢é…ç½®
    keywords: List[str] = None
    city: str = "å…¨å›½"
    salary_range: str = ""  # ä¾‹å¦‚: "20-50"
    experience: str = ""  # ç»éªŒè¦æ±‚
    max_pages: int = 3
    items_per_page: int = 30
    
    # åçˆ¬è™«é…ç½®
    min_delay: float = 2.0
    max_delay: float = 5.0
    mouse_move_enabled: bool = True
    max_retries: int = 3
    
    # æ•°æ®å­˜å‚¨
    output_dir: str = "data"
    save_html: bool = True
    save_csv: bool = True
    save_json: bool = True
    
    def __post_init__(self):
        if self.viewport is None:
            self.viewport = {'width': 1920, 'height': 1080}
        if self.keywords is None:
            self.keywords = ["Python"]
        os.makedirs(self.output_dir, exist_ok=True)


# selectors.py - é€‰æ‹©å™¨é…ç½®
class Selectors:
    """ç»Ÿä¸€ç®¡ç†æ‰€æœ‰é€‰æ‹©å™¨ï¼ˆé˜²æ­¢é¡µé¢æ”¹ç‰ˆåéš¾ä»¥ç»´æŠ¤ï¼‰"""
    
    # æœç´¢ç›¸å…³
    SEARCH_BOX = [".ipt-search", "input[name='query']", "#search-input"]
    SEARCH_BUTTON = [".btn-search", "button[type='submit']", ".search-btn"]
    
    # å¼¹çª—å…³é—­
    CLOSE_BUTTONS = [".btn-close", ".dialog-close", ".close-btn", "[class*='close']"]
    
    # èŒä½åˆ—è¡¨
    JOB_LIST = [".job-list", ".job-list-box", "[class*='job-list']"]
    JOB_CARD = [".job-card-wrapper", ".job-card", ".job-primary", "[class*='job-card']"]
    
    # èŒä½è¯¦æƒ…
    JOB_TITLE = [".job-title", ".job-name", "a.job-name", ".info-primary h3", "span.job-name"]
    JOB_LINK = ["a", ".job-card-left a"]
    SALARY = [".salary", ".red", "[class*='salary']", ".job-limit .red", "span.salary"]
    COMPANY_NAME = [".company-name", ".name", "h3.name", ".company-text h3", "span.company-name"]
    
    # æ ‡ç­¾ä¿¡æ¯
    TAG_LIST = [".tag-list", ".job-tags", ".info-labels"]
    EXPERIENCE = [".tag-list li:first-child", "[class*='experience']"]
    EDUCATION = [".tag-list li:nth-child(2)", "[class*='education']"]
    LOCATION = [".job-area", ".area", "[class*='location']"]
    
    # ç¦åˆ©å¾…é‡
    WELFARE = [".info-desc", ".tag-list", "[class*='welfare']"]
    
    # å…¬å¸ä¿¡æ¯
    COMPANY_INFO = [".company-tag-list", ".company-info", ".info-company"]
    
    # åˆ†é¡µ
    NEXT_PAGE = [".next", ".page-next", "[class*='next']"]
    PAGE_NUMBER = [".page-number", ".cur", ".active"]


# utils.py - å·¥å…·å‡½æ•°
import time
import random
import re
import json
from datetime import datetime
from typing import Optional, List
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('spider.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Utils:
    """å·¥å…·ç±»"""
    
    @staticmethod
    def random_sleep(min_sec: float = 1, max_sec: float = 3):
        """éšæœºå»¶è¿Ÿ"""
        time.sleep(random.uniform(min_sec, max_sec))
    
    @staticmethod
    def human_mouse_move(page):
        """æ¨¡æ‹Ÿäººç±»é¼ æ ‡ç§»åŠ¨"""
        for _ in range(random.randint(2, 4)):
            x = random.randint(100, 1200)
            y = random.randint(100, 800)
            page.mouse.move(x, y, steps=random.randint(5, 15))
            time.sleep(random.uniform(0.1, 0.3))
    
    @staticmethod
    def validate_salary(salary: str) -> bool:
        """éªŒè¯è–ªèµ„æ ¼å¼"""
        if not salary or salary == "é¢è®®":
            return True
        # åŒ¹é…æ ¼å¼: "10-20K", "10K-20K", "1-2ä¸‡", æˆ–è€…åŒ…å«æ•°å­—
        # æ”¾å®½éªŒè¯ï¼šåªè¦åŒ…å«æ•°å­—æˆ–Kå°±è®¤ä¸ºæœ‰æ•ˆ
        return bool(re.search(r'\d+', salary)) or 'K' in salary.upper()
    
    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    @staticmethod
    def parse_salary(salary: str) -> Dict:
        """è§£æè–ªèµ„èŒƒå›´"""
        if not salary or salary == "é¢è®®":
            return {"min": 0, "max": 0, "unit": "é¢è®®"}
        
        # æå–æ•°å­—
        numbers = re.findall(r'\d+', salary)
        
        if len(numbers) >= 2:
            min_val = int(numbers[0])
            max_val = int(numbers[1])
            
            # åˆ¤æ–­å•ä½
            if 'K' in salary.upper():
                unit = "K"
            elif 'ä¸‡' in salary:
                unit = "ä¸‡"
                min_val *= 10
                max_val *= 10
            else:
                unit = "K"
            
            return {"min": min_val, "max": max_val, "unit": unit, "avg": (min_val + max_val) / 2}
        elif len(numbers) == 1:
            # åªæœ‰ä¸€ä¸ªæ•°å­—çš„æƒ…å†µï¼Œæ¯”å¦‚ "15K"
            val = int(numbers[0])
            if 'K' in salary.upper():
                unit = "K"
            elif 'ä¸‡' in salary:
                unit = "ä¸‡"
                val *= 10
            else:
                unit = "K"
            return {"min": val, "max": val, "unit": unit, "avg": val}
        
        # æ— æ³•è§£æï¼Œä½†ä¿ç•™åŸå§‹å­—ç¬¦ä¸²
        return {"min": 0, "max": 0, "unit": "æœªçŸ¥", "raw": salary}
    
    @staticmethod
    def save_to_json(data: List[Dict], filename: str):
        """ä¿å­˜ä¸º JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"å·²ä¿å­˜ JSON: {filename}")
    
    @staticmethod
    def save_to_csv(data: List[Dict], filename: str):
        """ä¿å­˜ä¸º CSV"""
        import pandas as pd
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        logger.info(f"å·²ä¿å­˜ CSV: {filename}")


# parser.py - æ•°æ®è§£æå™¨
class JobParser:
    """èŒä½ä¿¡æ¯è§£æå™¨"""
    
    @staticmethod
    def safe_get_text(element, selectors: List[str], default: str = "") -> str:
        """å®‰å…¨è·å–æ–‡æœ¬ï¼ˆå°è¯•å¤šä¸ªé€‰æ‹©å™¨ï¼‰"""
        for selector in selectors:
            try:
                elem = element.query_selector(selector)
                if elem:
                    text = elem.inner_text().strip()
                    if text:
                        return Utils.clean_text(text)
            except Exception as e:
                continue
        return default
    
    @staticmethod
    def safe_get_attribute(element, selectors: List[str], attr: str, default: str = "") -> str:
        """å®‰å…¨è·å–å±æ€§"""
        for selector in selectors:
            try:
                elem = element.query_selector(selector)
                if elem:
                    value = elem.get_attribute(attr)
                    if value:
                        return value
            except:
                continue
        return default
    
    @classmethod
    def parse_job_card(cls, card, debug=False) -> Optional[Dict]:
        """è§£æå•ä¸ªèŒä½å¡ç‰‡"""
        try:
            # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°å¡ç‰‡HTML
            if debug:
                logger.debug(f"Card HTML: {card.inner_html()[:500]}")
            
            # åŸºç¡€ä¿¡æ¯
            title = cls.safe_get_text(card, Selectors.JOB_TITLE, "æœªçŸ¥èŒä½")
            link = cls.safe_get_attribute(card, Selectors.JOB_LINK, "href", "")
            if link and not link.startswith("http"):
                link = f"https://www.zhipin.com{link}"
            
            company = cls.safe_get_text(card, Selectors.COMPANY_NAME, "æœªçŸ¥å…¬å¸")
            salary = cls.safe_get_text(card, Selectors.SALARY, "é¢è®®")
            
            # å¦‚æœæ²¡æŠ“åˆ°è–ªèµ„ï¼Œå°è¯•ä»æ•´ä¸ªå¡ç‰‡æ–‡æœ¬ä¸­æå–
            if salary == "é¢è®®" or not salary:
                card_text = card.inner_text()
                # å°è¯•åŒ¹é…è–ªèµ„æ¨¡å¼
                salary_match = re.search(r'(\d+[-~]\d+K|\d+K[-~]\d+K|\d+[-~]\d+ä¸‡)', card_text)
                if salary_match:
                    salary = salary_match.group(1)
            
            # æ ‡ç­¾ä¿¡æ¯
            experience = cls.safe_get_text(card, Selectors.EXPERIENCE)
            education = cls.safe_get_text(card, Selectors.EDUCATION)
            location = cls.safe_get_text(card, Selectors.LOCATION)
            
            # ç¦åˆ©å¾…é‡
            welfare = cls.safe_get_text(card, Selectors.WELFARE)
            
            # å…¬å¸ä¿¡æ¯
            company_info = cls.safe_get_text(card, Selectors.COMPANY_INFO)
            
            # è§£æè–ªèµ„
            salary_parsed = Utils.parse_salary(salary)
            
            job_data = {
                "èŒä½åç§°": title,
                "å…¬å¸åç§°": company,
                "è–ªèµ„": salary,
                "è–ªèµ„æœ€ä½": salary_parsed.get("min", 0),
                "è–ªèµ„æœ€é«˜": salary_parsed.get("max", 0),
                "è–ªèµ„å¹³å‡": salary_parsed.get("avg", 0),
                "ç»éªŒè¦æ±‚": experience,
                "å­¦å†è¦æ±‚": education,
                "å·¥ä½œåœ°ç‚¹": location,
                "ç¦åˆ©å¾…é‡": welfare,
                "å…¬å¸ä¿¡æ¯": company_info,
                "èŒä½é“¾æ¥": link,
                "æŠ“å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # æ•°æ®éªŒè¯
            if cls.validate_job_data(job_data):
                return job_data
            else:
                logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥: {title} | è–ªèµ„: {salary}")
                if debug:
                    logger.debug(f"Job data: {job_data}")
                return None
                
        except Exception as e:
            logger.error(f"è§£æèŒä½å¡ç‰‡å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def validate_job_data(job: Dict) -> bool:
        """éªŒè¯èŒä½æ•°æ®å®Œæ•´æ€§"""
        required_fields = ["èŒä½åç§°", "å…¬å¸åç§°"]
        
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        for field in required_fields:
            if not job.get(field) or job[field] in ["æœªçŸ¥èŒä½", "æœªçŸ¥å…¬å¸", ""]:
                return False
        
        # æ”¾å®½è–ªèµ„éªŒè¯ï¼šåªè¦ä¸æ˜¯ç©ºçš„å°±è¡Œ
        if not job.get("è–ªèµ„"):
            return False
        
        return True


# spider.py - ä¸»çˆ¬è™«ç±»
from playwright.sync_api import sync_playwright, Page, Browser
from typing import List, Dict

class BossSpider:
    """Boss ç›´è˜çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, config: SpiderConfig = None):
        self.config = config or SpiderConfig()
        self.jobs: List[Dict] = []
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.stats = {
            "total_crawled": 0,
            "total_valid": 0,
            "total_failed": 0,
            "start_time": None,
            "end_time": None
        }
    
    def setup_browser(self, playwright):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        logger.info("æ­£åœ¨å¯åŠ¨ Chrome æµè§ˆå™¨...")
        
        self.browser = playwright.chromium.launch(
            headless=self.config.headless,
            executable_path=self.config.chrome_path,
            args=[
                '--no-sandbox',
                '--disable-blink-features=AutomationControlled',
                '--start-maximized',
                '--disable-dev-shm-usage'
            ]
        )
        
        context = self.browser.new_context(
            viewport=self.config.viewport,
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='zh-CN',
            timezone_id='Asia/Shanghai',
            permissions=[]
        )
        
        self.page = context.new_page()
        
        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => false});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
            Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en']});
            window.chrome = {
                runtime: {},
                app: {},
                csi: function() {},
                loadTimes: function() {}
            };
        """)
        
        logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
    
    def close_popups(self):
        """å…³é—­å¯èƒ½å‡ºç°çš„å¼¹çª—"""
        for selector in Selectors.CLOSE_BUTTONS:
            try:
                self.page.click(selector, timeout=2000)
                logger.info("å·²å…³é—­å¼¹çª—")
                Utils.random_sleep(0.5, 1)
            except:
                continue
    
    def search_jobs(self, keyword: str, first_search: bool = True):
        """æœç´¢èŒä½"""
        logger.info(f"æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
        
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡æœç´¢ï¼Œå…ˆå›åˆ°é¦–é¡µ
        if not first_search:
            logger.info("è¿”å›é¦–é¡µé‡æ–°æœç´¢...")
            self.page.goto("https://www.zhipin.com", timeout=30000)
            Utils.random_sleep(3, 5)
            self.close_popups()
        
        # å°è¯•å¤šä¸ªæœç´¢æ¡†é€‰æ‹©å™¨
        search_box = None
        for selector in Selectors.SEARCH_BOX:
            try:
                search_box = self.page.wait_for_selector(selector, timeout=10000)
                if search_box:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°æœç´¢æ¡†")
                    break
            except:
                continue
        
        if not search_box:
            # ä¿å­˜é¡µé¢ç”¨äºè°ƒè¯•
            with open(f"{self.config.output_dir}/debug_search_page.html", "w", encoding="utf-8") as f:
                f.write(self.page.content())
            raise Exception("æœªæ‰¾åˆ°æœç´¢æ¡†ï¼Œå·²ä¿å­˜é¡µé¢åˆ° debug_search_page.html")
        
        # æ¸…ç©ºå¹¶è¾“å…¥å…³é”®è¯
        search_box.click()
        Utils.random_sleep(0.5, 1)
        
        # æ¸…ç©ºè¾“å…¥æ¡† - ä½¿ç”¨å¤šç§æ–¹æ³•
        search_box.fill("")
        self.page.keyboard.press("Control+A")
        self.page.keyboard.press("Backspace")
        Utils.random_sleep(0.3, 0.6)
        
        # æ¨¡æ‹Ÿé€å­—è¾“å…¥
        for char in keyword:
            search_box.type(char, delay=random.randint(50, 150))
        
        Utils.random_sleep(1, 2)
        
        # ç‚¹å‡»æœç´¢æŒ‰é’®
        for selector in Selectors.SEARCH_BUTTON:
            try:
                self.page.click(selector, timeout=5000)
                logger.info("å·²ç‚¹å‡»æœç´¢æŒ‰é’®")
                break
            except:
                continue
        
        # ç­‰å¾…ç»“æœåŠ è½½
        Utils.random_sleep(5, 8)
        
        try:
            self.page.wait_for_selector(Selectors.JOB_LIST[0], timeout=15000)
            logger.info("èŒä½åˆ—è¡¨åŠ è½½å®Œæˆ")
        except:
            logger.warning("ç­‰å¾…èŒä½åˆ—è¡¨è¶…æ—¶ï¼Œå°è¯•ç»§ç»­...")
    
    def crawl_current_page(self, debug=False) -> int:
        """æŠ“å–å½“å‰é¡µé¢çš„èŒä½"""
        # ä¿å­˜é¡µé¢æºç ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if self.config.save_html or debug:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_file = f"{self.config.output_dir}/page_{timestamp}.html"
            with open(html_file, "w", encoding="utf-8") as f:
                f.write(self.page.content())
            logger.info(f"å·²ä¿å­˜é¡µé¢æºç : {html_file}")
        
        # æŸ¥æ‰¾èŒä½å¡ç‰‡
        job_cards = []
        for selector in Selectors.JOB_CARD:
            try:
                job_cards = self.page.query_selector_all(selector)
                if job_cards:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(job_cards)} ä¸ªèŒä½")
                    break
            except:
                continue
        
        if not job_cards:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•èŒä½å¡ç‰‡")
            # å¦‚æœç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œå¼€å¯è°ƒè¯•æ¨¡å¼ä¿å­˜å‰å‡ ä¸ªå…ƒç´ 
            if debug:
                with open(f"{self.config.output_dir}/debug_elements.txt", "w", encoding="utf-8") as f:
                    f.write("Page title: " + self.page.title() + "\n\n")
                    f.write("Page URL: " + self.page.url + "\n\n")
            return 0
        
        count = 0
        for i, card in enumerate(job_cards[:self.config.items_per_page], 1):
            self.stats["total_crawled"] += 1
            
            # å‰3ä¸ªå¡ç‰‡å¼€å¯è°ƒè¯•
            job_data = JobParser.parse_job_card(card, debug=(i <= 3 and debug))
            if job_data:
                self.jobs.append(job_data)
                self.stats["total_valid"] += 1
                count += 1
                logger.info(f"[{i}/{len(job_cards)}] âœ“ {job_data['èŒä½åç§°']} @ {job_data['å…¬å¸åç§°']} - {job_data['è–ªèµ„']}")
            else:
                self.stats["total_failed"] += 1
                logger.warning(f"[{i}/{len(job_cards)}] âœ— è§£æå¤±è´¥")
        
        return count
    
    def go_to_next_page(self) -> bool:
        """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # æ»šåŠ¨åˆ°åº•éƒ¨
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            Utils.random_sleep(1, 2)
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
            for selector in Selectors.NEXT_PAGE:
                try:
                    next_btn = self.page.query_selector(selector)
                    if next_btn and next_btn.is_visible():
                        # æ£€æŸ¥æ˜¯å¦å¯ç‚¹å‡»
                        if "disabled" in (next_btn.get_attribute("class") or ""):
                            logger.info("å·²åˆ°æœ€åä¸€é¡µ")
                            return False
                        
                        next_btn.click()
                        logger.info("å·²ç‚¹å‡»ä¸‹ä¸€é¡µ")
                        Utils.random_sleep(5, 8)
                        return True
                except:
                    continue
            
            logger.warning("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
            return False
            
        except Exception as e:
            logger.error(f"ç¿»é¡µå¤±è´¥: {e}")
            return False
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        self.stats["start_time"] = datetime.now()
        logger.info("=" * 60)
        logger.info("Boss ç›´è˜çˆ¬è™«å¯åŠ¨")
        logger.info(f"æœç´¢å…³é”®è¯: {', '.join(self.config.keywords)}")
        logger.info(f"æœ€å¤§é¡µæ•°: {self.config.max_pages}")
        logger.info("=" * 60)
        
        with sync_playwright() as playwright:
            try:
                self.setup_browser(playwright)
                
                # æ‰“å¼€ Boss ç›´è˜
                logger.info("æ­£åœ¨è®¿é—® Boss ç›´è˜...")
                self.page.goto("https://www.zhipin.com", timeout=30000)
                Utils.random_sleep(3, 5)
                
                # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                if self.config.mouse_move_enabled:
                    Utils.human_mouse_move(self.page)
                
                # å…³é—­å¼¹çª—
                self.close_popups()
                
                # éå†å…³é”®è¯
                for idx, keyword in enumerate(self.config.keywords):
                    # æœç´¢
                    self.search_jobs(keyword, first_search=(idx == 0))
                    
                    # æŠ“å–å¤šé¡µ
                    for page_num in range(1, self.config.max_pages + 1):
                        logger.info(f"\n{'='*50}")
                        logger.info(f"å…³é”®è¯: {keyword} - ç¬¬ {page_num} é¡µ")
                        logger.info(f"{'='*50}")
                        
                        # ç¬¬ä¸€é¡µç¬¬ä¸€ä¸ªå…³é”®è¯å¼€å¯è°ƒè¯•
                        debug_mode = (idx == 0 and page_num == 1)
                        count = self.crawl_current_page(debug=debug_mode)
                        logger.info(f"æœ¬é¡µæŠ“å–: {count} æ¡æœ‰æ•ˆæ•°æ®")
                        
                        # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œç¿»é¡µ
                        if page_num < self.config.max_pages:
                            if not self.go_to_next_page():
                                logger.info("æ— æ³•ç»§ç»­ç¿»é¡µï¼Œåœæ­¢æŠ“å–")
                                break
                        
                        # éšæœºå»¶è¿Ÿ
                        Utils.random_sleep(self.config.min_delay, self.config.max_delay)
                
                # ä¿å­˜æ•°æ®
                self.save_results()
                
            except Exception as e:
                logger.error(f"çˆ¬è™«è¿è¡Œå‡ºé”™: {e}", exc_info=True)
            finally:
                self.stats["end_time"] = datetime.now()
                self.print_stats()
                
                input("\næŒ‰ Enter å…³é—­æµè§ˆå™¨...")
                if self.browser:
                    self.browser.close()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.jobs:
            logger.warning("æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ CSV
        if self.config.save_csv:
            csv_file = f"{self.config.output_dir}/boss_jobs_{timestamp}.csv"
            Utils.save_to_csv(self.jobs, csv_file)
        
        # ä¿å­˜ JSON
        if self.config.save_json:
            json_file = f"{self.config.output_dir}/boss_jobs_{timestamp}.json"
            Utils.save_to_json(self.jobs, json_file)
        
        logger.info(f"\nâœ… æ•°æ®ä¿å­˜å®Œæˆï¼å…± {len(self.jobs)} æ¡")
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"æ€»å…±æŠ“å–: {self.stats['total_crawled']} æ¡")
        logger.info(f"æœ‰æ•ˆæ•°æ®: {self.stats['total_valid']} æ¡")
        logger.info(f"å¤±è´¥æ•°æ®: {self.stats['total_failed']} æ¡")
        logger.info(f"æˆåŠŸç‡: {self.stats['total_valid']/max(self.stats['total_crawled'],1)*100:.1f}%")
        logger.info(f"è€—æ—¶: {duration:.1f} ç§’")
        logger.info("=" * 60)


# main.py - ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    import sys
    
    # æµ‹è¯•æ¨¡å¼ï¼šå¿«é€Ÿè°ƒè¯•é€‰æ‹©å™¨
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        print("ğŸ” æµ‹è¯•æ¨¡å¼ï¼šåªæŠ“å–ç¬¬ä¸€é¡µï¼Œå¼€å¯è¯¦ç»†æ—¥å¿—")
        config = SpiderConfig(
            keywords=["Python"],
            max_pages=1,
            items_per_page=5,  # åªæŠ“5æ¡
            headless=False,
            save_html=True,    # ä¿å­˜HTMLç”¨äºåˆ†æ
            save_csv=True,
            save_json=False,
            min_delay=2.0,
            max_delay=3.0
        )
    else:
        # æ­£å¸¸æ¨¡å¼
        config = SpiderConfig(
            keywords=["Python"],  # å¯ä»¥æœç´¢å¤šä¸ªå…³é”®è¯: ["Python", "æ•°æ®åˆ†æ", "Java"]
            max_pages=3,          # æ¯ä¸ªå…³é”®è¯æŠ“å– 3 é¡µ
            items_per_page=30,
            headless=False,       # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            min_delay=3.0,
            max_delay=6.0,
            save_csv=True,
            save_json=True,
            save_html=False       # ä¸ä¿å­˜é¡µé¢æºç ï¼ˆæ–‡ä»¶å¤ªå¤§ï¼‰
        )
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = BossSpider(config)
    
    # è¿è¡Œ
    spider.run()